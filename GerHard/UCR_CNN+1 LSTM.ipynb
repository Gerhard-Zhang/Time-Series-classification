{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import keras \n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter = ',')\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  \n",
    "foder = u\"E:/Jupyter File/UCR_TS_Archive_2015/\"\n",
    "\n",
    "#flist = ['Adiac', 'Beef', 'CBF', 'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', \n",
    "#'DiatomSizeReduction', 'ECGFiveDays', 'FaceAll', 'FaceFour', 'FacesUCR', '50words', 'FISH', 'Gun_Point', 'Haptics', \n",
    "#'InlineSkate', 'ItalyPowerDemand', 'Lighting2', 'Lighting7', 'MALLAT', 'MedicalImages', 'MoteStrain', 'NonInvasiveFatalECG_Thorax1', \n",
    "#'NonInvasiveFatalECG_Thorax2', 'OliveOil', 'OSULeaf', 'SonyAIBORobotSurface', 'SonyAIBORobotSurfaceII', 'StarLightCurves', 'SwedishLeaf', 'Symbols', \n",
    "#'synthetic_control', 'Trace', 'TwoLeadECG', 'Two_Patterns', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y', 'uWaveGestureLibrary_Z', 'wafer', 'WordsSynonyms', 'yoga']\n",
    "\n",
    "#flist  = ['MiddlePhalanxTW']\n",
    "#for each in flist:\n",
    "each = 'Adiac'\n",
    "\n",
    "fname = each\n",
    "x_train, y_train = readucr(foder+fname+'/'+fname+'_TRAIN')\n",
    "x_test, y_test = readucr(foder+fname+'/'+fname+'_TEST')\n",
    "\n",
    "###################################################################################################################\n",
    "'''模型变量'''\n",
    "nb_epochs = 40\n",
    "nb_feature = 0\n",
    "nb_classes = len(np.unique(y_test))\n",
    "nb_feature = x_train.shape[1]\n",
    "batch_size = min(x_train.shape[0]/10, 16)\n",
    "####################################################################################################################\n",
    "\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "x_test = x_test.reshape(x_test.shape + (1,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# x_train.shape:  (390, 176, 1, 1)\n",
      "# x_test.shape:  (391, 176, 1, 1)\n",
      "# Y_train.shape:  (390, 37)\n",
      "# Y_test.shape:  (391, 37)\n"
     ]
    }
   ],
   "source": [
    "print(\"# x_train.shape: \",x_train.shape)\n",
    "print(\"# x_test.shape: \",x_test.shape)\n",
    "\n",
    "print(\"# Y_train.shape: \",Y_train.shape)\n",
    "print(\"# Y_test.shape: \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_Sequential模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CNN'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# 全局变量\n",
    "batch_size = batch_size\n",
    "nb_classses = nb_classes\n",
    "epochs = nb_epochs\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (3,1)\n",
    "# convolution kernel size\n",
    "kernel_size = (8,1)\n",
    "kernel_size_2 = (3,1)\n",
    "input_shape = (x_train.shape[1:])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 390 samples, validate on 391 samples\n",
      "Epoch 1/40\n",
      "390/390 [==============================] - 1s - loss: 3.6379 - acc: 0.0154 - val_loss: 3.6149 - val_acc: 0.0179\n",
      "Epoch 2/40\n",
      "390/390 [==============================] - 1s - loss: 3.6022 - acc: 0.0538 - val_loss: 3.6102 - val_acc: 0.0358\n",
      "Epoch 3/40\n",
      "390/390 [==============================] - 1s - loss: 3.6029 - acc: 0.0487 - val_loss: 3.6088 - val_acc: 0.0409\n",
      "Epoch 4/40\n",
      "390/390 [==============================] - 1s - loss: 3.5801 - acc: 0.0385 - val_loss: 3.6019 - val_acc: 0.0742\n",
      "Epoch 5/40\n",
      "390/390 [==============================] - 1s - loss: 3.5829 - acc: 0.0462 - val_loss: 3.5862 - val_acc: 0.0716\n",
      "Epoch 6/40\n",
      "390/390 [==============================] - 1s - loss: 3.5657 - acc: 0.0615 - val_loss: 3.5840 - val_acc: 0.0997\n",
      "Epoch 7/40\n",
      "390/390 [==============================] - 1s - loss: 3.5539 - acc: 0.0769 - val_loss: 3.5517 - val_acc: 0.0742\n",
      "Epoch 8/40\n",
      "390/390 [==============================] - 1s - loss: 3.5228 - acc: 0.0795 - val_loss: 3.5141 - val_acc: 0.0946\n",
      "Epoch 9/40\n",
      "390/390 [==============================] - 1s - loss: 3.4958 - acc: 0.0872 - val_loss: 3.4883 - val_acc: 0.0716\n",
      "Epoch 10/40\n",
      "390/390 [==============================] - 1s - loss: 3.4582 - acc: 0.0897 - val_loss: 3.4299 - val_acc: 0.0946\n",
      "Epoch 11/40\n",
      "390/390 [==============================] - 1s - loss: 3.4549 - acc: 0.0897 - val_loss: 3.4078 - val_acc: 0.1049\n",
      "Epoch 12/40\n",
      "390/390 [==============================] - 1s - loss: 3.4131 - acc: 0.1026 - val_loss: 3.3909 - val_acc: 0.0691\n",
      "Epoch 13/40\n",
      "390/390 [==============================] - 1s - loss: 3.3659 - acc: 0.0974 - val_loss: 3.3531 - val_acc: 0.0921\n",
      "Epoch 14/40\n",
      "390/390 [==============================] - 1s - loss: 3.3503 - acc: 0.1077 - val_loss: 3.3264 - val_acc: 0.1407\n",
      "Epoch 15/40\n",
      "390/390 [==============================] - 1s - loss: 3.3496 - acc: 0.0923 - val_loss: 3.3069 - val_acc: 0.1483\n",
      "Epoch 16/40\n",
      "390/390 [==============================] - 1s - loss: 3.2648 - acc: 0.1077 - val_loss: 3.3032 - val_acc: 0.1279\n",
      "Epoch 17/40\n",
      "390/390 [==============================] - 1s - loss: 3.2524 - acc: 0.1179 - val_loss: 3.2472 - val_acc: 0.1279\n",
      "Epoch 18/40\n",
      "390/390 [==============================] - 1s - loss: 3.2625 - acc: 0.1128 - val_loss: 3.2131 - val_acc: 0.1381\n",
      "Epoch 19/40\n",
      "390/390 [==============================] - 1s - loss: 3.1822 - acc: 0.1385 - val_loss: 3.1863 - val_acc: 0.1611\n",
      "Epoch 20/40\n",
      "390/390 [==============================] - 1s - loss: 3.1861 - acc: 0.1462 - val_loss: 3.1353 - val_acc: 0.1355\n",
      "Epoch 21/40\n",
      "390/390 [==============================] - 1s - loss: 3.1746 - acc: 0.1385 - val_loss: 3.0890 - val_acc: 0.1714\n",
      "Epoch 22/40\n",
      "390/390 [==============================] - 1s - loss: 3.1223 - acc: 0.1538 - val_loss: 3.0567 - val_acc: 0.1739\n",
      "Epoch 23/40\n",
      "390/390 [==============================] - 1s - loss: 3.1199 - acc: 0.1564 - val_loss: 3.0720 - val_acc: 0.1662\n",
      "Epoch 24/40\n",
      "390/390 [==============================] - 1s - loss: 3.1338 - acc: 0.1538 - val_loss: 3.0646 - val_acc: 0.2020\n",
      "Epoch 25/40\n",
      "390/390 [==============================] - 1s - loss: 3.0346 - acc: 0.1821 - val_loss: 2.9872 - val_acc: 0.1739\n",
      "Epoch 26/40\n",
      "390/390 [==============================] - 1s - loss: 3.0846 - acc: 0.1436 - val_loss: 2.9907 - val_acc: 0.1662\n",
      "Epoch 27/40\n",
      "390/390 [==============================] - 1s - loss: 2.9931 - acc: 0.2026 - val_loss: 2.9183 - val_acc: 0.2046\n",
      "Epoch 28/40\n",
      "390/390 [==============================] - 1s - loss: 2.9984 - acc: 0.1846 - val_loss: 2.9167 - val_acc: 0.2276\n",
      "Epoch 29/40\n",
      "390/390 [==============================] - 1s - loss: 2.8955 - acc: 0.2103 - val_loss: 2.8511 - val_acc: 0.1841\n",
      "Epoch 30/40\n",
      "390/390 [==============================] - 1s - loss: 2.9363 - acc: 0.2051 - val_loss: 2.8937 - val_acc: 0.2430\n",
      "Epoch 31/40\n",
      "390/390 [==============================] - 1s - loss: 2.8699 - acc: 0.1897 - val_loss: 2.8593 - val_acc: 0.2020\n",
      "Epoch 32/40\n",
      "390/390 [==============================] - 1s - loss: 2.8368 - acc: 0.1974 - val_loss: 2.7829 - val_acc: 0.2353\n",
      "Epoch 33/40\n",
      "390/390 [==============================] - 1s - loss: 2.8171 - acc: 0.2333 - val_loss: 2.7419 - val_acc: 0.2148\n",
      "Epoch 34/40\n",
      "390/390 [==============================] - 1s - loss: 2.8009 - acc: 0.2205 - val_loss: 2.7146 - val_acc: 0.2762\n",
      "Epoch 35/40\n",
      "390/390 [==============================] - 1s - loss: 2.8130 - acc: 0.2103 - val_loss: 2.7218 - val_acc: 0.2711\n",
      "Epoch 36/40\n",
      "390/390 [==============================] - 1s - loss: 2.7479 - acc: 0.2359 - val_loss: 2.7315 - val_acc: 0.2174\n",
      "Epoch 37/40\n",
      "390/390 [==============================] - 1s - loss: 2.7498 - acc: 0.2308 - val_loss: 2.6511 - val_acc: 0.2634\n",
      "Epoch 38/40\n",
      "390/390 [==============================] - 1s - loss: 2.7116 - acc: 0.2333 - val_loss: 2.6401 - val_acc: 0.2788\n",
      "Epoch 39/40\n",
      "390/390 [==============================] - 1s - loss: 2.7003 - acc: 0.2436 - val_loss: 2.6502 - val_acc: 0.2762\n",
      "Epoch 40/40\n",
      "390/390 [==============================] - 1s - loss: 2.6185 - acc: 0.2615 - val_loss: 2.5859 - val_acc: 0.2839\n",
      "Test score: 2.58591876798\n",
      "Test accuracy: 0.283887468107\n"
     ]
    }
   ],
   "source": [
    "#构建模型  1\n",
    "model = Sequential()  \n",
    "\n",
    "model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1]), padding='same', input_shape=input_shape)) # 卷积层1  \n",
    "model.add(Activation('relu')) #激活层  \n",
    "\n",
    "model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1]))) #卷积层2  \n",
    "model.add(Activation('relu')) #激活层\n",
    "\n",
    "model.add(Convolution2D(128, (kernel_size_2[0], kernel_size_2[1]))) #卷积层3  \n",
    "model.add(Activation('relu')) #激活层\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=pool_size)) #池化层  \n",
    "model.add(Dropout(0.25)) #神经元随机失活  \n",
    "model.add(Flatten()) #拉成一维数据  \n",
    "model.add(Dense(128)) #全连接层1  \n",
    "model.add(Activation('relu')) #激活层  \n",
    "model.add(Dropout(0.5)) #随机失活  \n",
    "model.add(Dense(nb_classes)) #全连接层2  \n",
    "model.add(Activation('softmax')) #Softmax评分  \n",
    "  \n",
    "#编译模型  \n",
    "model.compile(loss='categorical_crossentropy',  \n",
    "              optimizer='adadelta',  \n",
    "              metrics=['accuracy'])  \n",
    "\n",
    "#训练模型  \n",
    "model.fit(x_train, Y_train, batch_size=batch_size, epochs=epochs,  \n",
    "          verbose=1, validation_data=(x_test, Y_test))  \n",
    "#评估模型  \n",
    "score = model.evaluate(x_test, Y_test, verbose=0)  \n",
    "print('Test score:', score[0])  \n",
    "print('Test accuracy:', score[1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_函数式模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 1), padding=\"same\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 1), padding=\"same\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 1), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "输出测试 conv1 —— feature map: \n",
      "Tensor(\"activation_49/Relu:0\", shape=(?, 176, 1, 128), dtype=float32)\n",
      "##################################\n",
      "输出测试 conv2 —— feature map: \n",
      "Tensor(\"activation_50/Relu:0\", shape=(?, 176, 1, 256), dtype=float32)\n",
      "##################################\n",
      "输出测试 conv3 —— feature map: \n",
      "Tensor(\"activation_51/Relu:0\", shape=(?, 176, 1, 128), dtype=float32)\n",
      "###################################\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e6cde7892f8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"###################################\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mfull\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mfull\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    " \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras \n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "filters_1 = 128\n",
    "filters_2 = 256\n",
    "filters_3 = 128\n",
    "\n",
    "kernel_size_1 = (8,1)\n",
    "kernel_size_2 = (5,1)\n",
    "kernel_size_3 = (3,1)\n",
    "\n",
    "x = keras.layers.Input(x_train.shape[1:])\n",
    "\n",
    "#    drop_out = Dropout(0.2)(x)\n",
    "conv1 = keras.layers.Conv2D(128, (8, 1), border_mode='same')(x)\n",
    "conv1 = keras.layers.normalization.BatchNormalization()(conv1)\n",
    "conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "print(\"##################################\")\n",
    "print(\"输出测试 conv1 —— feature map: \")\n",
    "print(conv1)\n",
    "print(\"##################################\")\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv1)\n",
    "conv2 = keras.layers.Conv2D(256, (5, 1), border_mode='same')(conv1)\n",
    "conv2 = keras.layers.normalization.BatchNormalization()(conv2)\n",
    "conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "print(\"输出测试 conv2 —— feature map: \")\n",
    "print(conv2)\n",
    "print(\"##################################\")\n",
    "\n",
    "#    drop_out = Dropout(0.2)(conv2)\n",
    "conv3 = keras.layers.Conv2D(128, (3, 1), border_mode='same')(conv2)\n",
    "conv3 = keras.layers.normalization.BatchNormalization()(conv3)\n",
    "conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "print(\"输出测试 conv3 —— feature map: \")\n",
    "print(conv3)\n",
    "print(\"###################################\")\n",
    "\n",
    "full = keras.layers.Reshape(conv3.shape[1], conv3.shape[2], conv3.shape[3])(conv3)\n",
    "full = LSTM(8, input_shape=(full.shape[1], full.shape[2]), return_sequences=True)(conv3)\n",
    "\n",
    "#full = keras.layers.pooling.GlobalAveragePooling2D()(conv3)\n",
    "\n",
    "out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
    "\n",
    "model = Model(input=x, output=out)\n",
    " \n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001) \n",
    "hist = model.fit(x_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n",
    "          verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "#Print the testing results which has the lowest training loss.\n",
    "log = pd.DataFrame(hist.history)\n",
    "print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape[1:] (176, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.60255907]],\n",
       "\n",
       "       [[ 1.60396307]],\n",
       "\n",
       "       [[ 1.57498062]],\n",
       "\n",
       "       [[ 1.55492356]],\n",
       "\n",
       "       [[ 1.5117006 ]],\n",
       "\n",
       "       [[ 1.43839205]],\n",
       "\n",
       "       [[ 1.37290575]],\n",
       "\n",
       "       [[ 1.30902402]],\n",
       "\n",
       "       [[ 1.21375299]],\n",
       "\n",
       "       [[ 1.11988595]],\n",
       "\n",
       "       [[ 1.02692149]],\n",
       "\n",
       "       [[ 0.92862184]],\n",
       "\n",
       "       [[ 0.83047262]],\n",
       "\n",
       "       [[ 0.74132902]],\n",
       "\n",
       "       [[ 0.64488466]],\n",
       "\n",
       "       [[ 0.55801754]],\n",
       "\n",
       "       [[ 0.46427084]],\n",
       "\n",
       "       [[ 0.37063446]],\n",
       "\n",
       "       [[ 0.27922442]],\n",
       "\n",
       "       [[ 0.18636023]],\n",
       "\n",
       "       [[ 0.09580462]],\n",
       "\n",
       "       [[ 0.01067644]],\n",
       "\n",
       "       [[-0.0810866 ]],\n",
       "\n",
       "       [[-0.16602223]],\n",
       "\n",
       "       [[-0.24418459]],\n",
       "\n",
       "       [[-0.33638689]],\n",
       "\n",
       "       [[-0.42732559]],\n",
       "\n",
       "       [[-0.49890924]],\n",
       "\n",
       "       [[-0.56846712]],\n",
       "\n",
       "       [[-0.65171394]],\n",
       "\n",
       "       [[-0.73433899]],\n",
       "\n",
       "       [[-0.78001895]],\n",
       "\n",
       "       [[-0.8453849 ]],\n",
       "\n",
       "       [[-0.91991693]],\n",
       "\n",
       "       [[-0.99078855]],\n",
       "\n",
       "       [[-1.05800978]],\n",
       "\n",
       "       [[-1.09992903]],\n",
       "\n",
       "       [[-1.15037254]],\n",
       "\n",
       "       [[-1.20181889]],\n",
       "\n",
       "       [[-1.24674671]],\n",
       "\n",
       "       [[-1.28525626]],\n",
       "\n",
       "       [[-1.3163447 ]],\n",
       "\n",
       "       [[-1.3395106 ]],\n",
       "\n",
       "       [[-1.35495454]],\n",
       "\n",
       "       [[-1.38082814]],\n",
       "\n",
       "       [[-1.39185953]],\n",
       "\n",
       "       [[-1.38082814]],\n",
       "\n",
       "       [[-1.36097166]],\n",
       "\n",
       "       [[-1.3333932 ]],\n",
       "\n",
       "       [[-1.29739078]],\n",
       "\n",
       "       [[-1.26128807]],\n",
       "\n",
       "       [[-1.23391019]],\n",
       "\n",
       "       [[-1.17754985]],\n",
       "\n",
       "       [[-1.11737868]],\n",
       "\n",
       "       [[-1.07505828]],\n",
       "\n",
       "       [[-1.00967227]],\n",
       "\n",
       "       [[-0.95163717]],\n",
       "\n",
       "       [[-0.87265247]],\n",
       "\n",
       "       [[-0.79242424]],\n",
       "\n",
       "       [[-0.72657691]],\n",
       "\n",
       "       [[-0.64467391]],\n",
       "\n",
       "       [[-0.59331781]],\n",
       "\n",
       "       [[-0.52287742]],\n",
       "\n",
       "       [[-0.43185849]],\n",
       "\n",
       "       [[-0.35185088]],\n",
       "\n",
       "       [[-0.27062982]],\n",
       "\n",
       "       [[-0.17413531]],\n",
       "\n",
       "       [[-0.08936716]],\n",
       "\n",
       "       [[-0.00393141]],\n",
       "\n",
       "       [[ 0.08569687]],\n",
       "\n",
       "       [[ 0.17567985]],\n",
       "\n",
       "       [[ 0.26782198]],\n",
       "\n",
       "       [[ 0.36400561]],\n",
       "\n",
       "       [[ 0.45367069]],\n",
       "\n",
       "       [[ 0.55067665]],\n",
       "\n",
       "       [[ 0.65167397]],\n",
       "\n",
       "       [[ 0.74836905]],\n",
       "\n",
       "       [[ 0.84807269]],\n",
       "\n",
       "       [[ 0.9490299 ]],\n",
       "\n",
       "       [[ 1.04868339]],\n",
       "\n",
       "       [[ 1.1495704 ]],\n",
       "\n",
       "       [[ 1.24594457]],\n",
       "\n",
       "       [[ 1.34783443]],\n",
       "\n",
       "       [[ 1.44591345]],\n",
       "\n",
       "       [[ 1.5450956 ]],\n",
       "\n",
       "       [[ 1.62873354]],\n",
       "\n",
       "       [[ 1.67436335]],\n",
       "\n",
       "       [[ 1.69000785]],\n",
       "\n",
       "       [[ 1.70154066]],\n",
       "\n",
       "       [[ 1.70234294]],\n",
       "\n",
       "       [[ 1.69221413]],\n",
       "\n",
       "       [[ 1.64317462]],\n",
       "\n",
       "       [[ 1.57818975]],\n",
       "\n",
       "       [[ 1.48281843]],\n",
       "\n",
       "       [[ 1.38082829]],\n",
       "\n",
       "       [[ 1.28024214]],\n",
       "\n",
       "       [[ 1.18035799]],\n",
       "\n",
       "       [[ 1.08077469]],\n",
       "\n",
       "       [[ 0.98199367]],\n",
       "\n",
       "       [[ 0.88304217]],\n",
       "\n",
       "       [[ 0.78014946]],\n",
       "\n",
       "       [[ 0.68358475]],\n",
       "\n",
       "       [[ 0.59211454]],\n",
       "\n",
       "       [[ 0.49415586]],\n",
       "\n",
       "       [[ 0.40549363]],\n",
       "\n",
       "       [[ 0.30947046]],\n",
       "\n",
       "       [[ 0.21412923]],\n",
       "\n",
       "       [[ 0.12303007]],\n",
       "\n",
       "       [[ 0.03771035]],\n",
       "\n",
       "       [[-0.052752  ]],\n",
       "\n",
       "       [[-0.13790224]],\n",
       "\n",
       "       [[-0.22102872]],\n",
       "\n",
       "       [[-0.3164602 ]],\n",
       "\n",
       "       [[-0.39573573]],\n",
       "\n",
       "       [[-0.44735257]],\n",
       "\n",
       "       [[-0.5349618 ]],\n",
       "\n",
       "       [[-0.60457985]],\n",
       "\n",
       "       [[-0.67533113]],\n",
       "\n",
       "       [[-0.73971429]],\n",
       "\n",
       "       [[-0.82136657]],\n",
       "\n",
       "       [[-0.88778552]],\n",
       "\n",
       "       [[-0.95279045]],\n",
       "\n",
       "       [[-1.00546029]],\n",
       "\n",
       "       [[-1.06603261]],\n",
       "\n",
       "       [[-1.12841006]],\n",
       "\n",
       "       [[-1.16912589]],\n",
       "\n",
       "       [[-1.20773573]],\n",
       "\n",
       "       [[-1.25206183]],\n",
       "\n",
       "       [[-1.27743401]],\n",
       "\n",
       "       [[-1.28806425]],\n",
       "\n",
       "       [[-1.27693258]],\n",
       "\n",
       "       [[-1.29037081]],\n",
       "\n",
       "       [[-1.29558564]],\n",
       "\n",
       "       [[-1.29227623]],\n",
       "\n",
       "       [[-1.28134513]],\n",
       "\n",
       "       [[-1.26168921]],\n",
       "\n",
       "       [[-1.23491304]],\n",
       "\n",
       "       [[-1.20071576]],\n",
       "\n",
       "       [[-1.15959878]],\n",
       "\n",
       "       [[-1.11366812]],\n",
       "\n",
       "       [[-1.08759394]],\n",
       "\n",
       "       [[-1.02872648]],\n",
       "\n",
       "       [[-0.96529603]],\n",
       "\n",
       "       [[-0.89762351]],\n",
       "\n",
       "       [[-0.85022868]],\n",
       "\n",
       "       [[-0.77474394]],\n",
       "\n",
       "       [[-0.6965515 ]],\n",
       "\n",
       "       [[-0.63380299]],\n",
       "\n",
       "       [[-0.55279253]],\n",
       "\n",
       "       [[-0.48597243]],\n",
       "\n",
       "       [[-0.40984587]],\n",
       "\n",
       "       [[-0.32534548]],\n",
       "\n",
       "       [[-0.24815589]],\n",
       "\n",
       "       [[-0.16411681]],\n",
       "\n",
       "       [[-0.07713236]],\n",
       "\n",
       "       [[ 0.00882638]],\n",
       "\n",
       "       [[ 0.09755159]],\n",
       "\n",
       "       [[ 0.18643043]],\n",
       "\n",
       "       [[ 0.28481031]],\n",
       "\n",
       "       [[ 0.38229764]],\n",
       "\n",
       "       [[ 0.46435107]],\n",
       "\n",
       "       [[ 0.55851896]],\n",
       "\n",
       "       [[ 0.65282725]],\n",
       "\n",
       "       [[ 0.74030611]],\n",
       "\n",
       "       [[ 0.83712154]],\n",
       "\n",
       "       [[ 0.93259314]],\n",
       "\n",
       "       [[ 1.02782405]],\n",
       "\n",
       "       [[ 1.12409794]],\n",
       "\n",
       "       [[ 1.22067267]],\n",
       "\n",
       "       [[ 1.31624456]],\n",
       "\n",
       "       [[ 1.40690247]],\n",
       "\n",
       "       [[ 1.48522528]],\n",
       "\n",
       "       [[ 1.5253394 ]],\n",
       "\n",
       "       [[ 1.56866265]],\n",
       "\n",
       "       [[ 1.57538176]],\n",
       "\n",
       "       [[ 1.59744452]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"x_train.shape[1:]\",x_train.shape[1:])\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# t_x_train.shape:  (100, 10)\n",
      "# t_x_test.shape:  (10, 10)\n",
      "# t_y_train.shape:  (100, 1)\n",
      "# t_y_test.shape:  (10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.48530653,  0.6814032 ,  0.40589211,  0.2037158 ,  0.71807214,\n",
       "         0.65999421,  0.91126138,  0.40701753,  0.60189854,  0.87693833],\n",
       "       [ 0.67842036,  0.26040035,  0.83243133,  0.47147041,  0.22289942,\n",
       "         0.51934516,  0.52626243,  0.41556139,  0.58881512,  0.10532688],\n",
       "       [ 0.19834607,  0.39457151,  0.21891345,  0.31202382,  0.40049868,\n",
       "         0.5606715 ,  0.3102617 ,  0.89832663,  0.95034988,  0.12046023],\n",
       "       [ 0.0836587 ,  0.17671227,  0.12188595,  0.83126026,  0.30768291,\n",
       "         0.62209125,  0.5681506 ,  0.46330067,  0.29300683,  0.42418595],\n",
       "       [ 0.285441  ,  0.39636956,  0.72358094,  0.45513391,  0.45833962,\n",
       "         0.62009232,  0.54833128,  0.33363549,  0.59364411,  0.93730216],\n",
       "       [ 0.15806044,  0.57582786,  0.91338765,  0.82912605,  0.54902452,\n",
       "         0.68220497,  0.2060992 ,  0.2293142 ,  0.32726642,  0.74304278],\n",
       "       [ 0.00464153,  0.08535252,  0.2613706 ,  0.03583547,  0.25484007,\n",
       "         0.08784926,  0.28345024,  0.79493829,  0.86575126,  0.90265091],\n",
       "       [ 0.13902883,  0.94672723,  0.70401762,  0.23176572,  0.04488626,\n",
       "         0.50362182,  0.03852197,  0.92933733,  0.20121852,  0.86879063],\n",
       "       [ 0.17214134,  0.22034193,  0.71538889,  0.62310275,  0.58604644,\n",
       "         0.11784522,  0.64304285,  0.47289878,  0.80738196,  0.84440744],\n",
       "       [ 0.54978205,  0.98552983,  0.84018666,  0.46255005,  0.72170643,\n",
       "         0.03271379,  0.56918799,  0.40031304,  0.10382111,  0.3226427 ],\n",
       "       [ 0.06533767,  0.40903353,  0.39105483,  0.11475639,  0.5206022 ,\n",
       "         0.43345978,  0.60642682,  0.36550841,  0.28320771,  0.50269819],\n",
       "       [ 0.57672439,  0.35138545,  0.45257243,  0.87208559,  0.02885059,\n",
       "         0.43023591,  0.78531594,  0.27333259,  0.29737366,  0.14479422],\n",
       "       [ 0.22617572,  0.79649919,  0.14063653,  0.47542821,  0.39659212,\n",
       "         0.01902882,  0.11143782,  0.05492536,  0.67777697,  0.94797208],\n",
       "       [ 0.14558178,  0.92711875,  0.96871212,  0.69707831,  0.95015479,\n",
       "         0.99979784,  0.20216365,  0.14635663,  0.83805071,  0.71930408],\n",
       "       [ 0.19422465,  0.98573229,  0.40198158,  0.32008484,  0.51528374,\n",
       "         0.39346523,  0.03945837,  0.94842197,  0.2178262 ,  0.60747994],\n",
       "       [ 0.09405705,  0.21507377,  0.79390547,  0.96086672,  0.68823482,\n",
       "         0.17534171,  0.8333308 ,  0.37144357,  0.81038098,  0.66598623],\n",
       "       [ 0.31555404,  0.25947205,  0.13826868,  0.67167412,  0.56266632,\n",
       "         0.11848013,  0.54499115,  0.08476863,  0.79672347,  0.05061648],\n",
       "       [ 0.95518026,  0.97226395,  0.86091248,  0.00141279,  0.54710963,\n",
       "         0.05427523,  0.43075662,  0.07069342,  0.59243589,  0.30098055],\n",
       "       [ 0.02343461,  0.85822964,  0.10136214,  0.02160441,  0.78451563,\n",
       "         0.07959898,  0.73603464,  0.82927391,  0.45407349,  0.68797021],\n",
       "       [ 0.92937304,  0.47648924,  0.91307643,  0.32353157,  0.48735234,\n",
       "         0.34636738,  0.38646944,  0.36438428,  0.45686394,  0.80896817],\n",
       "       [ 0.06380633,  0.14814954,  0.30841282,  0.36030164,  0.88007582,\n",
       "         0.85377209,  0.0714421 ,  0.93644183,  0.13870772,  0.99107797],\n",
       "       [ 0.43615405,  0.73741448,  0.48104445,  0.67260202,  0.15145866,\n",
       "         0.44137119,  0.06256517,  0.57410323,  0.00777023,  0.22582359],\n",
       "       [ 0.69754113,  0.84626575,  0.97352131,  0.40372339,  0.32689015,\n",
       "         0.83528625,  0.12477152,  0.14383671,  0.13254186,  0.95187833],\n",
       "       [ 0.67734852,  0.76228697,  0.50335594,  0.03703513,  0.51085529,\n",
       "         0.52047921,  0.06729277,  0.06027542,  0.77994697,  0.01057719],\n",
       "       [ 0.19890386,  0.65702064,  0.8067792 ,  0.03788398,  0.61498624,\n",
       "         0.93323086,  0.34999406,  0.18469004,  0.44011597,  0.08065799],\n",
       "       [ 0.02589043,  0.29080995,  0.10557637,  0.18221634,  0.74232079,\n",
       "         0.79154005,  0.64292902,  0.89438044,  0.20144159,  0.73196257],\n",
       "       [ 0.53062212,  0.35856093,  0.34902129,  0.8364085 ,  0.71998556,\n",
       "         0.81546106,  0.09985461,  0.52145306,  0.17022361,  0.31805056],\n",
       "       [ 0.09830983,  0.61603489,  0.10179607,  0.53793766,  0.94567346,\n",
       "         0.61278534,  0.02133739,  0.61465298,  0.78081381,  0.72972879],\n",
       "       [ 0.30165213,  0.5699092 ,  0.92687864,  0.50317383,  0.71295507,\n",
       "         0.55244273,  0.26506488,  0.02205454,  0.18285544,  0.48052225],\n",
       "       [ 0.82187245,  0.23202953,  0.92225559,  0.57539035,  0.08518834,\n",
       "         0.5964111 ,  0.38745445,  0.19905717,  0.91143447,  0.91742922],\n",
       "       [ 0.14923821,  0.69990534,  0.41645379,  0.74112734,  0.19457077,\n",
       "         0.89866328,  0.30458981,  0.08069488,  0.58937411,  0.12130767],\n",
       "       [ 0.75467991,  0.10005588,  0.93179608,  0.86158432,  0.34594178,\n",
       "         0.07337733,  0.23636227,  0.80737778,  0.6912228 ,  0.80837307],\n",
       "       [ 0.10063343,  0.5291541 ,  0.81960169,  0.58930052,  0.17848446,\n",
       "         0.52948651,  0.5711229 ,  0.12672648,  0.91448505,  0.85116881],\n",
       "       [ 0.66269917,  0.82670468,  0.81063653,  0.25329652,  0.07337304,\n",
       "         0.95144823,  0.693453  ,  0.69126755,  0.88602217,  0.02526027],\n",
       "       [ 0.31701317,  0.51318838,  0.0656897 ,  0.14491043,  0.28490022,\n",
       "         0.07141533,  0.81296914,  0.40867681,  0.06279249,  0.01958016],\n",
       "       [ 0.59166613,  0.07824793,  0.36074361,  0.13273949,  0.65600534,\n",
       "         0.28857801,  0.53251367,  0.94547787,  0.72749524,  0.13611559],\n",
       "       [ 0.97418705,  0.4417683 ,  0.98752534,  0.33370281,  0.30937579,\n",
       "         0.95883153,  0.95938309,  0.87988708,  0.2302733 ,  0.75245021],\n",
       "       [ 0.71145306,  0.5796724 ,  0.04457496,  0.83785778,  0.52682622,\n",
       "         0.14889835,  0.79905773,  0.79874058,  0.47901041,  0.77753061],\n",
       "       [ 0.03258164,  0.23500311,  0.14553162,  0.24743755,  0.03667031,\n",
       "         0.78221334,  0.56705451,  0.72796004,  0.22636998,  0.99068506],\n",
       "       [ 0.40354907,  0.81854609,  0.81280585,  0.71680235,  0.50763333,\n",
       "         0.81829862,  0.7473438 ,  0.6881506 ,  0.60384117,  0.26542309],\n",
       "       [ 0.66806941,  0.1352768 ,  0.33434099,  0.16137199,  0.79532615,\n",
       "         0.56484674,  0.88598709,  0.48975172,  0.71331586,  0.93528043],\n",
       "       [ 0.59417667,  0.99148809,  0.14548816,  0.84268139,  0.50945446,\n",
       "         0.95741417,  0.32343601,  0.36608431,  0.76336527,  0.23241505],\n",
       "       [ 0.60585557,  0.14909666,  0.5778964 ,  0.6514926 ,  0.33101455,\n",
       "         0.93848908,  0.87222043,  0.11485462,  0.53760705,  0.95198237],\n",
       "       [ 0.06836483,  0.05604627,  0.43629606,  0.34532453,  0.30356054,\n",
       "         0.49308713,  0.21823028,  0.05592025,  0.25730396,  0.68057705],\n",
       "       [ 0.67436235,  0.43811882,  0.45800277,  0.2023637 ,  0.64902222,\n",
       "         0.29536191,  0.52846496,  0.92696154,  0.51820385,  0.52094319],\n",
       "       [ 0.59083545,  0.15088943,  0.76231762,  0.015826  ,  0.43624778,\n",
       "         0.95548785,  0.2169805 ,  0.40296867,  0.90875168,  0.94515491],\n",
       "       [ 0.33154521,  0.18277315,  0.815953  ,  0.7226663 ,  0.23767884,\n",
       "         0.787928  ,  0.71136202,  0.10814656,  0.54765273,  0.06154895],\n",
       "       [ 0.29333249,  0.09339136,  0.44506137,  0.02861584,  0.78459931,\n",
       "         0.96484484,  0.44939883,  0.77957794,  0.65111964,  0.71680997],\n",
       "       [ 0.40445418,  0.04734625,  0.06723834,  0.77484534,  0.47535077,\n",
       "         0.06333343,  0.86468094,  0.31317916,  0.6805309 ,  0.09151577],\n",
       "       [ 0.70254038,  0.54379586,  0.62233461,  0.42237978,  0.93635771,\n",
       "         0.49055617,  0.71109647,  0.22732273,  0.07267509,  0.15674002],\n",
       "       [ 0.20387567,  0.86055369,  0.92138696,  0.57774147,  0.38867868,\n",
       "         0.27423073,  0.81504356,  0.39681196,  0.78251019,  0.45756256],\n",
       "       [ 0.70231897,  0.00191447,  0.34554062,  0.91896144,  0.94036455,\n",
       "         0.23395426,  0.75342323,  0.60086343,  0.03351189,  0.33203824],\n",
       "       [ 0.32329886,  0.34470085,  0.04290039,  0.17998549,  0.87075521,\n",
       "         0.38047846,  0.17550538,  0.06127347,  0.24979411,  0.66978211],\n",
       "       [ 0.52948785,  0.11967168,  0.91927431,  0.57337959,  0.47806873,\n",
       "         0.38609965,  0.34315056,  0.5163757 ,  0.23747482,  0.21231006],\n",
       "       [ 0.90923738,  0.26757124,  0.11313468,  0.19627502,  0.10778234,\n",
       "         0.98143386,  0.44443645,  0.33110227,  0.51326473,  0.72459529],\n",
       "       [ 0.66579408,  0.7802609 ,  0.16908847,  0.8808643 ,  0.67240281,\n",
       "         0.50258882,  0.60174682,  0.44675142,  0.32768577,  0.6245956 ],\n",
       "       [ 0.17986678,  0.41449305,  0.76033485,  0.31511922,  0.40138684,\n",
       "         0.65219242,  0.57788798,  0.14802451,  0.61231131,  0.53442431],\n",
       "       [ 0.21525212,  0.89660956,  0.9585681 ,  0.19944937,  0.22530883,\n",
       "         0.9000369 ,  0.51092373,  0.21237753,  0.61116303,  0.95793158],\n",
       "       [ 0.39918878,  0.8344053 ,  0.16942082,  0.50268601,  0.17639798,\n",
       "         0.64948937,  0.34313932,  0.825791  ,  0.86881772,  0.8992732 ],\n",
       "       [ 0.23814616,  0.45316967,  0.41364301,  0.75373079,  0.98665995,\n",
       "         0.12498571,  0.59289412,  0.24286684,  0.35180549,  0.42550471],\n",
       "       [ 0.68651635,  0.93905642,  0.98279404,  0.50631902,  0.60993028,\n",
       "         0.74256646,  0.37371036,  0.0727159 ,  0.71849501,  0.23955808],\n",
       "       [ 0.05362825,  0.51232985,  0.57827961,  0.89443637,  0.12239243,\n",
       "         0.91163759,  0.23956658,  0.54473249,  0.08459372,  0.1893706 ],\n",
       "       [ 0.31921282,  0.45987282,  0.77810956,  0.71367563,  0.12528723,\n",
       "         0.68561774,  0.05465984,  0.74831987,  0.50822043,  0.08797456],\n",
       "       [ 0.99791483,  0.74494029,  0.39428643,  0.28939094,  0.71754614,\n",
       "         0.82937427,  0.54518533,  0.61918728,  0.1418296 ,  0.55520278],\n",
       "       [ 0.5955081 ,  0.34646625,  0.21423838,  0.78782194,  0.94071863,\n",
       "         0.0253362 ,  0.21593756,  0.59630669,  0.2314016 ,  0.44900748],\n",
       "       [ 0.63847807,  0.02706157,  0.25545714,  0.82756258,  0.22057808,\n",
       "         0.28341761,  0.12211509,  0.79626044,  0.44791574,  0.05545156],\n",
       "       [ 0.20964564,  0.73074123,  0.41264724,  0.34914878,  0.8143691 ,\n",
       "         0.7505323 ,  0.37421454,  0.04827194,  0.04685752,  0.90971528],\n",
       "       [ 0.94883341,  0.67088773,  0.86823886,  0.17441497,  0.28722375,\n",
       "         0.96433586,  0.19170074,  0.08843503,  0.93783064,  0.39825776],\n",
       "       [ 0.70205454,  0.70733109,  0.20410732,  0.30215486,  0.13045579,\n",
       "         0.89158628,  0.69909711,  0.81037242,  0.73847673,  0.53458098],\n",
       "       [ 0.76551809,  0.88688335,  0.48153445,  0.08610465,  0.62388143,\n",
       "         0.83055807,  0.41886227,  0.85383625,  0.96006137,  0.58662798],\n",
       "       [ 0.60632295,  0.93167283,  0.41542303,  0.30730468,  0.98124303,\n",
       "         0.49731192,  0.13635359,  0.9039295 ,  0.73120977,  0.91200203],\n",
       "       [ 0.87989325,  0.02312438,  0.38065889,  0.45038503,  0.42140829,\n",
       "         0.80791481,  0.56206674,  0.11157977,  0.66334643,  0.33561147],\n",
       "       [ 0.28792839,  0.23905204,  0.77548279,  0.43770061,  0.94177553,\n",
       "         0.84083158,  0.67160128,  0.07939734,  0.35341651,  0.57853641],\n",
       "       [ 0.36157773,  0.15748105,  0.51835115,  0.9309573 ,  0.81331805,\n",
       "         0.14970402,  0.84087649,  0.25117755,  0.73328993,  0.74844658],\n",
       "       [ 0.82644394,  0.76501994,  0.88874065,  0.33151264,  0.042844  ,\n",
       "         0.90460026,  0.31205055,  0.42342678,  0.25399437,  0.44982303],\n",
       "       [ 0.33952257,  0.71742859,  0.62597655,  0.64474467,  0.86168825,\n",
       "         0.81481679,  0.857166  ,  0.93576881,  0.04866222,  0.57596527],\n",
       "       [ 0.48593962,  0.64100294,  0.67025809,  0.30557692,  0.24341396,\n",
       "         0.47018779,  0.02865088,  0.89195469,  0.20804279,  0.72397104],\n",
       "       [ 0.80551287,  0.52438731,  0.4589958 ,  0.74509751,  0.18881005,\n",
       "         0.4886736 ,  0.95730296,  0.23069869,  0.40585884,  0.65783495],\n",
       "       [ 0.46487998,  0.12779918,  0.98100564,  0.33676791,  0.48344766,\n",
       "         0.69864956,  0.16892134,  0.00468511,  0.10142634,  0.70074392],\n",
       "       [ 0.93008483,  0.61792223,  0.78518762,  0.16451998,  0.77629191,\n",
       "         0.45930706,  0.12196079,  0.48303307,  0.18773578,  0.32266731],\n",
       "       [ 0.62567328,  0.34026589,  0.51435872,  0.4488815 ,  0.44526921,\n",
       "         0.35106912,  0.40561843,  0.06390716,  0.83964537,  0.03670275],\n",
       "       [ 0.0412243 ,  0.94793802,  0.45585991,  0.61901155,  0.0223676 ,\n",
       "         0.51682825,  0.3822979 ,  0.85753665,  0.02939732,  0.73425426],\n",
       "       [ 0.15877125,  0.59533621,  0.74593494,  0.25440181,  0.46207995,\n",
       "         0.94068323,  0.14043172,  0.42150686,  0.77212006,  0.15009441],\n",
       "       [ 0.82048753,  0.48710301,  0.44723323,  0.70950219,  0.08652177,\n",
       "         0.13600261,  0.1537716 ,  0.40079348,  0.17943694,  0.8567723 ],\n",
       "       [ 0.93381139,  0.55203189,  0.37599227,  0.2069836 ,  0.94927974,\n",
       "         0.40696773,  0.82233722,  0.56966824,  0.54887505,  0.79408934],\n",
       "       [ 0.75735346,  0.25599833,  0.91973745,  0.70794535,  0.7145233 ,\n",
       "         0.77430805,  0.24683006,  0.94744179,  0.42616067,  0.42505871],\n",
       "       [ 0.25450241,  0.44623953,  0.17740236,  0.11331131,  0.07293745,\n",
       "         0.96202984,  0.90788993,  0.59907945,  0.87022224,  0.91660192],\n",
       "       [ 0.94297972,  0.66483585,  0.0607713 ,  0.89503079,  0.28291357,\n",
       "         0.17767935,  0.03303613,  0.2569744 ,  0.03248366,  0.94472392],\n",
       "       [ 0.10967673,  0.81834141,  0.21738629,  0.94116848,  0.28077526,\n",
       "         0.21593906,  0.81241356,  0.06345275,  0.2121037 ,  0.63601484],\n",
       "       [ 0.39400718,  0.4396551 ,  0.7667631 ,  0.09917273,  0.53159689,\n",
       "         0.40945168,  0.86201999,  0.71727789,  0.10724958,  0.50824646],\n",
       "       [ 0.21980686,  0.24726314,  0.63192095,  0.22626233,  0.61648872,\n",
       "         0.16102201,  0.97955451,  0.96914834,  0.08331919,  0.89618378],\n",
       "       [ 0.71004768,  0.93792067,  0.47843545,  0.49426213,  0.22573074,\n",
       "         0.45424362,  0.49101354,  0.41668257,  0.84241253,  0.89277227],\n",
       "       [ 0.8463687 ,  0.69851349,  0.17715014,  0.1462747 ,  0.35026705,\n",
       "         0.69020391,  0.50028188,  0.42677744,  0.90540908,  0.82471447],\n",
       "       [ 0.35907377,  0.69535699,  0.16515857,  0.11323018,  0.02412214,\n",
       "         0.09165469,  0.09341625,  0.45709103,  0.08456087,  0.22352319],\n",
       "       [ 0.89221962,  0.45134496,  0.57634696,  0.85116626,  0.19542771,\n",
       "         0.04440191,  0.92813983,  0.37773463,  0.91158525,  0.94650297],\n",
       "       [ 0.81960021,  0.12156294,  0.70511997,  0.45722984,  0.27576573,\n",
       "         0.07815011,  0.74285473,  0.44790228,  0.04102641,  0.8078983 ],\n",
       "       [ 0.52321431,  0.94248212,  0.74826755,  0.59240411,  0.24753145,\n",
       "         0.64124895,  0.57266385,  0.9431161 ,  0.63843214,  0.55453689],\n",
       "       [ 0.03838037,  0.86465405,  0.12271931,  0.0297435 ,  0.94124683,\n",
       "         0.04969773,  0.1715593 ,  0.73115155,  0.62605584,  0.58488161],\n",
       "       [ 0.74813966,  0.6927964 ,  0.59849627,  0.76917868,  0.51781291,\n",
       "         0.86673802,  0.45444343,  0.97365423,  0.43696017,  0.25189903],\n",
       "       [ 0.09579604,  0.29007263,  0.05711902,  0.10817086,  0.32795047,\n",
       "         0.41626068,  0.38489766,  0.5955022 ,  0.21650747,  0.47504406]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "t_x_train = np.random.random((100, 10))\n",
    "t_y_train = np.random.randint(2, size=(100, 1))\n",
    "t_x_test = np.random.random((10, 10))\n",
    "t_y_test = np.random.randint(2, size=(10, 1))\n",
    "\n",
    "print(\"# t_x_train.shape: \",t_x_train.shape)\n",
    "print(\"# t_x_test.shape: \",t_x_test.shape)\n",
    "\n",
    "print(\"# t_y_train.shape: \",t_y_train.shape)\n",
    "print(\"# t_y_test.shape: \",t_y_test.shape)\n",
    "\n",
    "t_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.63621156]]\n",
      "\n",
      " [[ 0.73850751]]\n",
      "\n",
      " [[ 0.72442556]]\n",
      "\n",
      " [[ 0.22950671]]\n",
      "\n",
      " [[ 0.26581665]]\n",
      "\n",
      " [[ 0.43718159]]\n",
      "\n",
      " [[ 0.40804502]]\n",
      "\n",
      " [[ 0.62224256]]\n",
      "\n",
      " [[ 0.8847127 ]]\n",
      "\n",
      " [[ 0.88246296]]\n",
      "\n",
      " [[ 0.76182877]]\n",
      "\n",
      " [[ 0.9895867 ]]\n",
      "\n",
      " [[ 0.19631997]]\n",
      "\n",
      " [[ 0.32312628]]\n",
      "\n",
      " [[ 0.17305013]]\n",
      "\n",
      " [[ 0.4389336 ]]\n",
      "\n",
      " [[ 0.10823884]]\n",
      "\n",
      " [[ 0.28657198]]\n",
      "\n",
      " [[ 0.78553889]]\n",
      "\n",
      " [[ 0.11555483]]\n",
      "\n",
      " [[ 0.70053189]]\n",
      "\n",
      " [[ 0.16701429]]\n",
      "\n",
      " [[ 0.365673  ]]\n",
      "\n",
      " [[ 0.65889432]]\n",
      "\n",
      " [[ 0.08175094]]\n",
      "\n",
      " [[ 0.76160098]]\n",
      "\n",
      " [[ 0.92487546]]\n",
      "\n",
      " [[ 0.3648803 ]]\n",
      "\n",
      " [[ 0.52538441]]\n",
      "\n",
      " [[ 0.06350893]]\n",
      "\n",
      " [[ 0.72104547]]\n",
      "\n",
      " [[ 0.14341888]]\n",
      "\n",
      " [[ 0.6674371 ]]\n",
      "\n",
      " [[ 0.80368959]]\n",
      "\n",
      " [[ 0.90459924]]\n",
      "\n",
      " [[ 0.68994845]]\n",
      "\n",
      " [[ 0.7091976 ]]\n",
      "\n",
      " [[ 0.88611276]]\n",
      "\n",
      " [[ 0.69100987]]\n",
      "\n",
      " [[ 0.95488823]]\n",
      "\n",
      " [[ 0.83090261]]\n",
      "\n",
      " [[ 0.15250495]]\n",
      "\n",
      " [[ 0.90302959]]\n",
      "\n",
      " [[ 0.82297775]]\n",
      "\n",
      " [[ 0.54742358]]\n",
      "\n",
      " [[ 0.81500486]]\n",
      "\n",
      " [[ 0.72165166]]\n",
      "\n",
      " [[ 0.99590354]]\n",
      "\n",
      " [[ 0.73815579]]\n",
      "\n",
      " [[ 0.98971747]]\n",
      "\n",
      " [[ 0.12836376]]\n",
      "\n",
      " [[ 0.29835141]]\n",
      "\n",
      " [[ 0.99352019]]\n",
      "\n",
      " [[ 0.32406991]]\n",
      "\n",
      " [[ 0.47585849]]\n",
      "\n",
      " [[ 0.66914947]]\n",
      "\n",
      " [[ 0.14418508]]\n",
      "\n",
      " [[ 0.77465771]]\n",
      "\n",
      " [[ 0.81803258]]\n",
      "\n",
      " [[ 0.46880582]]\n",
      "\n",
      " [[ 0.6383634 ]]\n",
      "\n",
      " [[ 0.71785847]]\n",
      "\n",
      " [[ 0.89522265]]\n",
      "\n",
      " [[ 0.32066281]]\n",
      "\n",
      " [[ 0.56793335]]\n",
      "\n",
      " [[ 0.77649009]]\n",
      "\n",
      " [[ 0.42152295]]\n",
      "\n",
      " [[ 0.14291937]]\n",
      "\n",
      " [[ 0.24358154]]\n",
      "\n",
      " [[ 0.75845974]]\n",
      "\n",
      " [[ 0.94517169]]\n",
      "\n",
      " [[ 0.30963975]]\n",
      "\n",
      " [[ 0.48552932]]\n",
      "\n",
      " [[ 0.1132376 ]]\n",
      "\n",
      " [[ 0.47212333]]\n",
      "\n",
      " [[ 0.95168563]]\n",
      "\n",
      " [[ 0.17698304]]\n",
      "\n",
      " [[ 0.66581061]]\n",
      "\n",
      " [[ 0.34988812]]\n",
      "\n",
      " [[ 0.6769896 ]]\n",
      "\n",
      " [[ 0.82535443]]\n",
      "\n",
      " [[ 0.87186009]]\n",
      "\n",
      " [[ 0.42039195]]\n",
      "\n",
      " [[ 0.6914294 ]]\n",
      "\n",
      " [[ 0.53120115]]\n",
      "\n",
      " [[ 0.67421168]]\n",
      "\n",
      " [[ 0.96236444]]\n",
      "\n",
      " [[ 0.19674801]]\n",
      "\n",
      " [[ 0.55386216]]\n",
      "\n",
      " [[ 0.8579668 ]]\n",
      "\n",
      " [[ 0.80538671]]\n",
      "\n",
      " [[ 0.82902016]]\n",
      "\n",
      " [[ 0.53327498]]\n",
      "\n",
      " [[ 0.14693345]]\n",
      "\n",
      " [[ 0.3426512 ]]\n",
      "\n",
      " [[ 0.74559182]]\n",
      "\n",
      " [[ 0.00637401]]\n",
      "\n",
      " [[ 0.35555958]]\n",
      "\n",
      " [[ 0.62168159]]\n",
      "\n",
      " [[ 0.07325237]]\n",
      "\n",
      " [[ 0.29901656]]\n",
      "\n",
      " [[ 0.02514314]]\n",
      "\n",
      " [[ 0.68126664]]\n",
      "\n",
      " [[ 0.71768759]]\n",
      "\n",
      " [[ 0.12046621]]\n",
      "\n",
      " [[ 0.79457598]]\n",
      "\n",
      " [[ 0.01723743]]\n",
      "\n",
      " [[ 0.17457513]]\n",
      "\n",
      " [[ 0.16023826]]\n",
      "\n",
      " [[ 0.84147672]]\n",
      "\n",
      " [[ 0.22318433]]\n",
      "\n",
      " [[ 0.38220205]]\n",
      "\n",
      " [[ 0.42728412]]\n",
      "\n",
      " [[ 0.11382054]]\n",
      "\n",
      " [[ 0.87458597]]\n",
      "\n",
      " [[ 0.03314119]]\n",
      "\n",
      " [[ 0.65328493]]\n",
      "\n",
      " [[ 0.95300857]]\n",
      "\n",
      " [[ 0.48738507]]\n",
      "\n",
      " [[ 0.1287242 ]]\n",
      "\n",
      " [[ 0.67746747]]\n",
      "\n",
      " [[ 0.31623092]]\n",
      "\n",
      " [[ 0.41472366]]\n",
      "\n",
      " [[ 0.71319284]]\n",
      "\n",
      " [[ 0.3903233 ]]\n",
      "\n",
      " [[ 0.04917902]]\n",
      "\n",
      " [[ 0.99100763]]\n",
      "\n",
      " [[ 0.66335242]]\n",
      "\n",
      " [[ 0.24333019]]\n",
      "\n",
      " [[ 0.77578309]]\n",
      "\n",
      " [[ 0.24120791]]\n",
      "\n",
      " [[ 0.90993136]]\n",
      "\n",
      " [[ 0.13531958]]\n",
      "\n",
      " [[ 0.09289208]]\n",
      "\n",
      " [[ 0.79494669]]\n",
      "\n",
      " [[ 0.83940236]]\n",
      "\n",
      " [[ 0.93457378]]\n",
      "\n",
      " [[ 0.66738238]]\n",
      "\n",
      " [[ 0.98409213]]\n",
      "\n",
      " [[ 0.87387513]]\n",
      "\n",
      " [[ 0.02719641]]\n",
      "\n",
      " [[ 0.87125514]]\n",
      "\n",
      " [[ 0.92405545]]\n",
      "\n",
      " [[ 0.55379013]]\n",
      "\n",
      " [[ 0.1085532 ]]\n",
      "\n",
      " [[ 0.07115056]]\n",
      "\n",
      " [[ 0.34873228]]\n",
      "\n",
      " [[ 0.6886729 ]]\n",
      "\n",
      " [[ 0.7167226 ]]\n",
      "\n",
      " [[ 0.53124931]]\n",
      "\n",
      " [[ 0.01127718]]\n",
      "\n",
      " [[ 0.41132595]]\n",
      "\n",
      " [[ 0.19579209]]\n",
      "\n",
      " [[ 0.60654478]]\n",
      "\n",
      " [[ 0.52036838]]\n",
      "\n",
      " [[ 0.22971514]]\n",
      "\n",
      " [[ 0.23927024]]\n",
      "\n",
      " [[ 0.4493416 ]]\n",
      "\n",
      " [[ 0.36598345]]\n",
      "\n",
      " [[ 0.50298978]]\n",
      "\n",
      " [[ 0.53253417]]\n",
      "\n",
      " [[ 0.53085463]]\n",
      "\n",
      " [[ 0.19655899]]\n",
      "\n",
      " [[ 0.18060502]]\n",
      "\n",
      " [[ 0.09984663]]\n",
      "\n",
      " [[ 0.35572433]]\n",
      "\n",
      " [[ 0.86786142]]\n",
      "\n",
      " [[ 0.61523959]]\n",
      "\n",
      " [[ 0.97961159]]\n",
      "\n",
      " [[ 0.35784746]]\n",
      "\n",
      " [[ 0.45463766]]\n",
      "\n",
      " [[ 0.26570484]]\n",
      "\n",
      " [[ 0.3700984 ]]\n",
      "\n",
      " [[ 0.65164059]]\n",
      "\n",
      " [[ 0.01699807]]\n",
      "\n",
      " [[ 0.95874514]]]\n",
      "[[[ 0.87238371  0.50671836  0.56358964  0.83669787  0.47173298]]\n",
      "\n",
      " [[ 0.81633299  0.3225771   0.73164456  0.16270493  0.89526467]]\n",
      "\n",
      " [[ 0.50118752  0.03788198  0.9148809   0.08253558  0.77935025]]\n",
      "\n",
      " [[ 0.69986936  0.24191069  0.1815333   0.69005198  0.61608669]]\n",
      "\n",
      " [[ 0.6252503   0.95690483  0.58187977  0.96730484  0.23015401]]\n",
      "\n",
      " [[ 0.8104849   0.21252122  0.11037293  0.59003786  0.18758702]]\n",
      "\n",
      " [[ 0.91511733  0.25657332  0.16795338  0.49569039  0.31545647]]\n",
      "\n",
      " [[ 0.62629142  0.59901072  0.51303503  0.42175082  0.19706794]]\n",
      "\n",
      " [[ 0.05550268  0.83876993  0.00127681  0.89628438  0.16410539]]\n",
      "\n",
      " [[ 0.52709359  0.46943272  0.4270783   0.97890575  0.45012567]]\n",
      "\n",
      " [[ 0.26859936  0.16413189  0.10979686  0.02799892  0.96346478]]\n",
      "\n",
      " [[ 0.00190196  0.4176582   0.85380829  0.52924741  0.32687142]]\n",
      "\n",
      " [[ 0.02086867  0.96804871  0.38112193  0.07667559  0.38582727]]\n",
      "\n",
      " [[ 0.59688692  0.68181722  0.31590706  0.04220012  0.67965965]]\n",
      "\n",
      " [[ 0.53512127  0.08973429  0.1062146   0.59987761  0.26274826]]\n",
      "\n",
      " [[ 0.52695609  0.68414739  0.70074286  0.98751993  0.5528327 ]]\n",
      "\n",
      " [[ 0.93430508  0.15551324  0.39045462  0.44008385  0.89342334]]\n",
      "\n",
      " [[ 0.13328704  0.2269884   0.67153624  0.02075776  0.09347571]]\n",
      "\n",
      " [[ 0.13495162  0.90129713  0.7547772   0.09024792  0.98797883]]\n",
      "\n",
      " [[ 0.42832022  0.10341652  0.27304537  0.02515719  0.14804426]]\n",
      "\n",
      " [[ 0.64377599  0.22300643  0.40345217  0.98707768  0.40277276]]\n",
      "\n",
      " [[ 0.60636177  0.5545269   0.27222389  0.60159196  0.31865011]]\n",
      "\n",
      " [[ 0.08131901  0.44670241  0.46932225  0.40170091  0.26108571]]\n",
      "\n",
      " [[ 0.1033936   0.83335121  0.68938166  0.62017462  0.23916472]]\n",
      "\n",
      " [[ 0.69542413  0.7756779   0.10441433  0.90022905  0.01841015]]\n",
      "\n",
      " [[ 0.13158046  0.71507291  0.57246947  0.53268453  0.33077103]]\n",
      "\n",
      " [[ 0.81144958  0.41260731  0.28674154  0.09597327  0.73542385]]\n",
      "\n",
      " [[ 0.24419792  0.74723919  0.45297448  0.19643497  0.54191145]]\n",
      "\n",
      " [[ 0.36847108  0.82046959  0.36611465  0.45998397  0.23273842]]\n",
      "\n",
      " [[ 0.1857941   0.91763691  0.72975378  0.03639596  0.19091158]]\n",
      "\n",
      " [[ 0.30429766  0.89558385  0.38057052  0.66247158  0.92156586]]\n",
      "\n",
      " [[ 0.70503066  0.72665377  0.89733698  0.51550016  0.47174679]]\n",
      "\n",
      " [[ 0.52822294  0.30602469  0.19948254  0.58293739  0.92518043]]\n",
      "\n",
      " [[ 0.94834862  0.12163338  0.78517149  0.77500774  0.37932049]]\n",
      "\n",
      " [[ 0.97930092  0.63751858  0.38418716  0.7882528   0.46659003]]\n",
      "\n",
      " [[ 0.93084247  0.9060499   0.30367491  0.05476653  0.9248586 ]]\n",
      "\n",
      " [[ 0.47427901  0.4383969   0.96056451  0.40832254  0.56577074]]\n",
      "\n",
      " [[ 0.72964468  0.53990773  0.97857338  0.91240299  0.90125826]]\n",
      "\n",
      " [[ 0.13044917  0.77126677  0.16130037  0.13911083  0.50121643]]\n",
      "\n",
      " [[ 0.88093693  0.2552311   0.72007051  0.10384482  0.92353066]]\n",
      "\n",
      " [[ 0.95754602  0.8184628   0.60072234  0.16513624  0.71107584]]\n",
      "\n",
      " [[ 0.3033109   0.68122312  0.74424132  0.24708797  0.93087693]]\n",
      "\n",
      " [[ 0.92092476  0.32316349  0.75795019  0.15929108  0.09715115]]\n",
      "\n",
      " [[ 0.9908768   0.1003001   0.81756175  0.96116338  0.62595372]]\n",
      "\n",
      " [[ 0.44397553  0.61596992  0.99490116  0.62434608  0.4588547 ]]\n",
      "\n",
      " [[ 0.96888601  0.89381538  0.81199384  0.32461559  0.40401695]]\n",
      "\n",
      " [[ 0.76233836  0.14431233  0.16418766  0.18860925  0.00317431]]\n",
      "\n",
      " [[ 0.50130536  0.80037341  0.80754895  0.73337638  0.63006426]]\n",
      "\n",
      " [[ 0.2405779   0.6633692   0.75564288  0.54888262  0.6080352 ]]\n",
      "\n",
      " [[ 0.10386515  0.84300922  0.77429508  0.14516698  0.65716198]]\n",
      "\n",
      " [[ 0.22270094  0.06990486  0.80291075  0.98930467  0.59310145]]\n",
      "\n",
      " [[ 0.33555167  0.51022679  0.05368192  0.65645478  0.00546998]]\n",
      "\n",
      " [[ 0.45134518  0.55636642  0.39159215  0.66210668  0.71800168]]\n",
      "\n",
      " [[ 0.44113082  0.0406134   0.15489042  0.41730627  0.10793216]]\n",
      "\n",
      " [[ 0.44865831  0.07221829  0.52112197  0.06513573  0.78595481]]\n",
      "\n",
      " [[ 0.9738655   0.98226102  0.62439423  0.33520006  0.1349754 ]]\n",
      "\n",
      " [[ 0.21216746  0.60412328  0.57864284  0.43845745  0.15295554]]\n",
      "\n",
      " [[ 0.00930149  0.10970334  0.46740244  0.19313685  0.78319548]]\n",
      "\n",
      " [[ 0.76576932  0.93706145  0.89395025  0.34052114  0.59572695]]\n",
      "\n",
      " [[ 0.88664827  0.90791803  0.09039713  0.43298442  0.07461595]]\n",
      "\n",
      " [[ 0.13010722  0.79791552  0.46822813  0.30444157  0.80538953]]\n",
      "\n",
      " [[ 0.37844032  0.95212679  0.78455838  0.31907235  0.29501599]]\n",
      "\n",
      " [[ 0.05159616  0.3442425   0.93811697  0.68269174  0.13601493]]\n",
      "\n",
      " [[ 0.28541984  0.85453237  0.06799674  0.50905467  0.26226305]]\n",
      "\n",
      " [[ 0.6419405   0.20307463  0.6858276   0.45013323  0.81927038]]\n",
      "\n",
      " [[ 0.50449104  0.94989617  0.92823467  0.74231635  0.8848959 ]]\n",
      "\n",
      " [[ 0.91434845  0.77058356  0.74274369  0.08220722  0.22112701]]\n",
      "\n",
      " [[ 0.06179456  0.92275154  0.25038979  0.0270495   0.18496941]]\n",
      "\n",
      " [[ 0.71442968  0.42257551  0.04654293  0.3027845   0.76759248]]\n",
      "\n",
      " [[ 0.24872139  0.33849229  0.23641146  0.76267783  0.0095983 ]]\n",
      "\n",
      " [[ 0.04821367  0.3168944   0.28707995  0.94600585  0.51639244]]\n",
      "\n",
      " [[ 0.0944606   0.03471502  0.1861041   0.11044511  0.07774594]]\n",
      "\n",
      " [[ 0.01606869  0.03631701  0.20652778  0.62626586  0.8103656 ]]\n",
      "\n",
      " [[ 0.69373594  0.20475799  0.81525254  0.07282178  0.24642123]]\n",
      "\n",
      " [[ 0.38630762  0.76532827  0.10108353  0.51691557  0.79292964]]\n",
      "\n",
      " [[ 0.09373859  0.44674484  0.66242561  0.75011631  0.62944753]]\n",
      "\n",
      " [[ 0.64303056  0.9331279   0.09856548  0.34108327  0.7687421 ]]\n",
      "\n",
      " [[ 0.06159136  0.53369824  0.98125868  0.46651154  0.63761687]]\n",
      "\n",
      " [[ 0.33856779  0.70083144  0.06176549  0.86298186  0.54632274]]\n",
      "\n",
      " [[ 0.72012258  0.3465072   0.28314088  0.41866716  0.69707171]]\n",
      "\n",
      " [[ 0.43503579  0.63653048  0.71310105  0.65169692  0.82961095]]\n",
      "\n",
      " [[ 0.22702003  0.89209234  0.05001581  0.59802987  0.4542826 ]]\n",
      "\n",
      " [[ 0.49592953  0.93129404  0.27641441  0.61031469  0.2410818 ]]\n",
      "\n",
      " [[ 0.23102596  0.47328668  0.54304455  0.06820837  0.68633555]]\n",
      "\n",
      " [[ 0.21226172  0.8999482   0.37704408  0.95709935  0.87911147]]\n",
      "\n",
      " [[ 0.76413372  0.39722678  0.27643532  0.49585771  0.62857873]]\n",
      "\n",
      " [[ 0.83133728  0.62223554  0.39024898  0.77518267  0.44640468]]\n",
      "\n",
      " [[ 0.49724051  0.95502003  0.6044309   0.74897906  0.28147674]]\n",
      "\n",
      " [[ 0.84137415  0.5254602   0.35502343  0.28712646  0.50923256]]\n",
      "\n",
      " [[ 0.7573193   0.18541673  0.836825    0.81380967  0.5686138 ]]\n",
      "\n",
      " [[ 0.69761695  0.29351795  0.60294705  0.55656325  0.35544923]]\n",
      "\n",
      " [[ 0.50988482  0.23345151  0.00842913  0.04769679  0.16781096]]\n",
      "\n",
      " [[ 0.56419816  0.26035456  0.72980835  0.64725112  0.9900843 ]]\n",
      "\n",
      " [[ 0.66297659  0.00304158  0.50614461  0.85668677  0.28370326]]\n",
      "\n",
      " [[ 0.55408656  0.30334062  0.72638117  0.88161626  0.61821922]]\n",
      "\n",
      " [[ 0.27154601  0.19297023  0.11410411  0.58050729  0.31132719]]\n",
      "\n",
      " [[ 0.51831293  0.40090444  0.65843375  0.18814363  0.91365296]]\n",
      "\n",
      " [[ 0.50392399  0.66360346  0.14468398  0.56093416  0.33506945]]\n",
      "\n",
      " [[ 0.89117652  0.29762642  0.36503055  0.04576689  0.15468651]]\n",
      "\n",
      " [[ 0.95935994  0.88917191  0.42918825  0.66362999  0.75711723]]\n",
      "\n",
      " [[ 0.61901932  0.91612455  0.76381133  0.24716282  0.05451672]]\n",
      "\n",
      " [[ 0.1751293   0.3530403   0.59453378  0.70367009  0.85557496]]\n",
      "\n",
      " [[ 0.16719161  0.90095373  0.31743405  0.47677815  0.46768887]]\n",
      "\n",
      " [[ 0.83606405  0.77939066  0.81472942  0.28237282  0.42505208]]\n",
      "\n",
      " [[ 0.89848855  0.32038038  0.77987813  0.13590735  0.41517539]]\n",
      "\n",
      " [[ 0.98625496  0.8045223   0.42567753  0.92882745  0.45321977]]\n",
      "\n",
      " [[ 0.43901243  0.86195373  0.8510758   0.03308132  0.52175208]]\n",
      "\n",
      " [[ 0.88243779  0.44363054  0.25547956  0.05784177  0.20717931]]\n",
      "\n",
      " [[ 0.01724355  0.11095343  0.06820343  0.97809007  0.13739486]]\n",
      "\n",
      " [[ 0.53336103  0.83155312  0.2251181   0.18179352  0.57167736]]\n",
      "\n",
      " [[ 0.54829098  0.34677322  0.01721032  0.62289185  0.2371446 ]]\n",
      "\n",
      " [[ 0.94889422  0.06463135  0.03811228  0.30033705  0.37490128]]\n",
      "\n",
      " [[ 0.50308547  0.61549145  0.62333714  0.55913603  0.74156784]]\n",
      "\n",
      " [[ 0.9733433   0.48003733  0.5391149   0.0033495   0.01333432]]\n",
      "\n",
      " [[ 0.89661531  0.08738716  0.52138016  0.32018703  0.01905006]]\n",
      "\n",
      " [[ 0.27651676  0.92693043  0.62223787  0.77356435  0.86767874]]\n",
      "\n",
      " [[ 0.78088881  0.99854583  0.37160142  0.1515635   0.15204119]]\n",
      "\n",
      " [[ 0.05414399  0.90944481  0.11541126  0.71039213  0.59663463]]\n",
      "\n",
      " [[ 0.97230436  0.06004638  0.66661903  0.08941508  0.22991083]]\n",
      "\n",
      " [[ 0.95938928  0.45282782  0.16492882  0.75180262  0.74315587]]\n",
      "\n",
      " [[ 0.01805549  0.3715649   0.13586037  0.61668458  0.21818678]]\n",
      "\n",
      " [[ 0.99742155  0.46299686  0.85559349  0.26267045  0.18704875]]\n",
      "\n",
      " [[ 0.37184011  0.65479147  0.37019107  0.02774561  0.28443844]]\n",
      "\n",
      " [[ 0.99183732  0.85763744  0.40729416  0.4445246   0.06120985]]\n",
      "\n",
      " [[ 0.20274993  0.89134519  0.6208404   0.18759705  0.06886962]]\n",
      "\n",
      " [[ 0.82207074  0.00380113  0.42498176  0.00421684  0.75005773]]\n",
      "\n",
      " [[ 0.5157245   0.59367248  0.13358916  0.0320729   0.59162234]]\n",
      "\n",
      " [[ 0.20898344  0.05693145  0.63143241  0.7893583   0.91278264]]\n",
      "\n",
      " [[ 0.87169551  0.7900681   0.17258709  0.84652173  0.65089529]]\n",
      "\n",
      " [[ 0.30242671  0.5042982   0.70908212  0.8213028   0.699652  ]]\n",
      "\n",
      " [[ 0.56251359  0.00740717  0.76243898  0.58559471  0.57472943]]\n",
      "\n",
      " [[ 0.17299236  0.76504058  0.76511348  0.84043747  0.14463418]]\n",
      "\n",
      " [[ 0.04019747  0.11400553  0.74938219  0.28866612  0.81005286]]\n",
      "\n",
      " [[ 0.66442437  0.07620462  0.02824796  0.76143518  0.29234989]]\n",
      "\n",
      " [[ 0.36998463  0.38498847  0.56420388  0.9768516   0.99644163]]\n",
      "\n",
      " [[ 0.45060882  0.35271368  0.61757194  0.65674903  0.40518153]]\n",
      "\n",
      " [[ 0.6161495   0.01173417  0.36814988  0.66757932  0.13854562]]\n",
      "\n",
      " [[ 0.05874701  0.23223014  0.98538891  0.51969714  0.75185457]]\n",
      "\n",
      " [[ 0.88229643  0.05059675  0.99759045  0.27511146  0.48585409]]\n",
      "\n",
      " [[ 0.03239164  0.71417763  0.84956854  0.84783433  0.73830523]]\n",
      "\n",
      " [[ 0.36709011  0.02702553  0.85443076  0.86235388  0.67979258]]\n",
      "\n",
      " [[ 0.65704834  0.77144049  0.76904959  0.81971926  0.69779867]]\n",
      "\n",
      " [[ 0.58775705  0.97618314  0.74349903  0.97215199  0.52189169]]\n",
      "\n",
      " [[ 0.09942699  0.51433495  0.15918522  0.7571191   0.3597278 ]]\n",
      "\n",
      " [[ 0.27841414  0.01453232  0.00803713  0.6241176   0.46974174]]\n",
      "\n",
      " [[ 0.42024893  0.95025615  0.24613435  0.0731648   0.42657948]]\n",
      "\n",
      " [[ 0.73787931  0.08611252  0.32719066  0.84439801  0.56573542]]\n",
      "\n",
      " [[ 0.82567246  0.1379197   0.69706148  0.6904602   0.4769018 ]]\n",
      "\n",
      " [[ 0.32440011  0.74139239  0.87775339  0.45995733  0.21934221]]\n",
      "\n",
      " [[ 0.52669209  0.15934737  0.11890135  0.29899856  0.01845123]]\n",
      "\n",
      " [[ 0.15431735  0.46430926  0.00936142  0.61718585  0.08239447]]\n",
      "\n",
      " [[ 0.04463432  0.6834927   0.37727987  0.60735116  0.44981602]]\n",
      "\n",
      " [[ 0.06531527  0.37844896  0.97060113  0.97580719  0.75542431]]\n",
      "\n",
      " [[ 0.86253197  0.06824641  0.34309748  0.12753555  0.28503805]]\n",
      "\n",
      " [[ 0.81304939  0.98663524  0.59762899  0.84085393  0.81258421]]\n",
      "\n",
      " [[ 0.36560523  0.09092643  0.08296757  0.89961996  0.38507178]]\n",
      "\n",
      " [[ 0.80769159  0.40539047  0.03778227  0.50053277  0.09593697]]\n",
      "\n",
      " [[ 0.15766754  0.87235598  0.27376388  0.76111848  0.20704473]]\n",
      "\n",
      " [[ 0.05585174  0.19739502  0.64462776  0.29370498  0.62341575]]\n",
      "\n",
      " [[ 0.31887076  0.83858428  0.44821851  0.22350556  0.51293224]]\n",
      "\n",
      " [[ 0.80296077  0.63827595  0.60166693  0.91881183  0.90657495]]\n",
      "\n",
      " [[ 0.77477118  0.95509388  0.61799683  0.56351132  0.64823007]]\n",
      "\n",
      " [[ 0.77375104  0.03821514  0.77926931  0.29160687  0.42792744]]\n",
      "\n",
      " [[ 0.57636154  0.53858483  0.88030739  0.07312752  0.56568467]]\n",
      "\n",
      " [[ 0.02668554  0.52899406  0.16155672  0.66860696  0.1884986 ]]\n",
      "\n",
      " [[ 0.06872108  0.31676118  0.52984118  0.67168022  0.48980444]]\n",
      "\n",
      " [[ 0.38274857  0.32485256  0.56833543  0.04210312  0.0144143 ]]\n",
      "\n",
      " [[ 0.39673702  0.16828067  0.18683373  0.36010547  0.28386851]]\n",
      "\n",
      " [[ 0.04269731  0.00268566  0.06271477  0.65593679  0.44668926]]\n",
      "\n",
      " [[ 0.34062774  0.60659865  0.10463023  0.82312122  0.33548145]]\n",
      "\n",
      " [[ 0.96303334  0.16094151  0.20118985  0.51303483  0.32824184]]\n",
      "\n",
      " [[ 0.35392914  0.91467476  0.74303495  0.32451845  0.87529855]]\n",
      "\n",
      " [[ 0.1819957   0.20709249  0.88098093  0.86481558  0.98398164]]\n",
      "\n",
      " [[ 0.84646231  0.75526704  0.65228093  0.25907363  0.75479133]]\n",
      "\n",
      " [[ 0.67181734  0.3416461   0.2081711   0.8627023   0.10532479]]\n",
      "\n",
      " [[ 0.12572754  0.29526177  0.4906136   0.24748269  0.0861329 ]]]\n"
     ]
    }
   ],
   "source": [
    "t2_x_train = np.array(np.random.random((176,1,1)))\n",
    "t3_x_train = np.array(np.random.random((176,1,5)))\n",
    "\n",
    "print(t2_x_train)\n",
    "print(t3_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 176, 1, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([ [[1,2,3,4,5]],\n",
    "               [[11,22,33,44,55]],\n",
    "               [[111,222,333,444,555,666]],\n",
    "               [[1111,2222,3333,4444,5555,6666]]\n",
    "             ])\n",
    "a.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
